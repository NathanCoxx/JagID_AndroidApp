# JagID_AndroidApp

Jag ID
Overview
Jag Id is a mobile application which reads a user's face and unlocks a lock (such as a front door). This app is developed for an android user. Ideally, this would be available in the Google Play store. Once downloaded, they would need to sign-up and take a video using their phone camera. In the future, the user can unlock the lock by logging in and placing the phone’s camera in front of their face. The lock will automatically unlock if the camera recognizes the user.
Development Process
	The development process was centered around the use of SCRUM to develop the product iteratively. Each sprint had its own meeting sessions scheduled and own set of tasks to complete. Everyone was assigned work to do each sprint.
In the first meeting, we shared all the ideas about the potential product. Nathan shared that we should use machine learning to allocate the funds for charity, and others brought different ideas.  After a couple of meetings, we decided on building a facial recognition app to unlock a lock (such as front door). We also had to decide on what languages to use to develop this product and the general requirements of this product.
Next meeting, we discussed design ideas. After the design was finalized, we divided the design in the form of user stories. The following meetings would be about the three questions we discussed in class.
 Each feature was implemented separately and tested separately before merging into one whole product. Machine learning was tested with pictures of a celebrity and member’s pictures. The API was tested using Postman by sending dummy data in get and post requests. 
After deploying this product in the Google Play store we will try to maintain the server up with maximum downtime of two hours in every six months for new feature update. Deploying to Google Play was not an original user story and was not intended to be completed for this project.

Implementation
For this project, we used Python, OpenCV for machine learning, Androidx for the mobile application, Express for the server, and Node.js and JavaScript for restful API. We used a raspberry pi to demonstrate the unlocking mechanism. If anyone wants to try this app, he/she must download all the scripts and dependencies from the GitLab (link will be given below). One should use an IDE to run the androidx code and/or deploy the code to an Android device.
Mobile App
Our mobile application is written in Java and XML using the Android Studio IDE. The application features a login framework that starts with a LoginActivity class and LoginActivity XML layout. This class accepts a username and password, or gives the user an opportunity to register if they have not already. 
If the user is already registered, the application stores the username and password as variables and runs a background async task called LoginTask. This class uses the Android Volley Library to send a GET request to our API. It then reads the response and if the response equals “Login successful” then it returns a true boolean value to the LoginActivity, telling it to send the user to the Main Activity. 
If the user has not already registered, they will be sent to the Registration Activity. This activity will accept their name, username, email, and password, and run the same process of an async background task, called Registration Task,  to send this data to the api. 
  Once in the main activity, the user will have the option to “unlock,”  or “add user.” If they have not previously submitted any training photos, they will select the “add user” button. This button will open the camera function on the phone, and prompt the user to take a picture. It will then upload the image to our server with the use of RetroFit, and an OkHttpClient, sending a POST request.  This POST request will be sent to the “trainingupload” endpoint in our API. If the user has already submitted training data, they must simply choose the “unlock” button. Which will open the camera function, and use the same process to send a POST request to our “verifyupload” API endpoint. 
API
Originally, the file server and the API to service the database were two different servers running at the same time (both created using Node.js). During the second sprint, Jacob decided it would be a good idea to combine the two applications into one.
The API is currently acting as the “brain” of the system. It waits for the mobile app to send requests to its endpoints and then executes the proper script based on what endpoint was hit. The API makes sure that the requests are coming from a user registered in the MySQL database by using “session” variables. These session variables remember the last user that was queried successfully from the database.
	Almost all the endpoints on the API requires a user to login before becoming available. After authentication, the API will use the node package “multer” to handle file upload from the mobile app and the node package “python-shell” to handle running the Python scripts. The API creates directories for every new user to store the training data created when calling the Python scripts. At each point when images and/or video are no longer required, the API will delete the images/videos to prevent malicious use by external threats. Finally, the API interfaces with the raspberry Pi to send an unlock signal if the user is allowed in. The mobile app also receives confirmation or denial of entry.
Python Machine Learning Scripts
 	The training script essentially runs through your “profile” folders containing pictures of said profile’s face. Then, it creates our training data file to be used in our facial recognition script. Our facial recognition script now interacts with a saved video rather than a continuous stream of webcam video. The code on GitLab notes the change through the use of the OpenCV cv2 library’s VideoCapture function. This portion of the code below is essentially just comparing the pixel values from the face in the video to that of our training data file from our training script. If the user is recognized, the code returns “true” to the API. Lastly, the code kills the video analysis after 5 seconds.

Raspberry Pi
We are using Secure Socket Shell (SSH) to transmit the accept/deny signal to the Pi. We run a Python script in the background from the API to invoke the SSH. We run commands on your Raspberry Pi without needing to plug in a display, keyboard, mouse and having to move yourself to the location of your Raspberry Pi each time. Raspberry Pi uses TCP/IP to receive the signals. We also made sure there was no firewall blocking the communication. The Pi should light up if an accept signal is received. Lighting up is our way of simulating a lock opening.
Difficulties
The major difficulties were that our project was written in three different languages and that every group member had more than one project this semester. The languages we had to learn were Java, Python and JavaScript. Each member had different projects for their classes which made it hard to find time to get together for a meeting.
 To overcome learning several languages, we split the learning of new languages across the entire group. Tommy was comfortable with Python and did the facial recognition training. Trevor and Jacob knew basic JavaScript so they setup the Express server and API. Jacob was familiar with MySQL and setup the database. Nathan had taken a mobile development class and worked on the mobile application in Java.
The main concern was how these different language will communicate with each other. We had some issues with mobile application communicating with server (even with having APIs designed for it). While we figured out get and post requests in Java, we used the Postman application to test the API. We also had to figure out how to integrate the Python scripts into the system. We ended up having the machine learning be handled server-side to make the system less complicated to use on Android.
We used the Discord application for most of our online meetings. Discord helped to overcome the problem of not everyone having time to meet on a regular basis. If any member had difficulties, the member shared his screen and we helped him to find the solution. On a regular basis we used GroupMe to update any changes and share any difficulties. 
Missing Features
	Due to an unforeseen conflict, the mobile app and the API are not currently able to communicate with each other. The API saves session variables using cookies to remember who is logged in. We were unaware that the mobile portion of the project does not handle cookies. This means the mobile app is able to reach the endpoints on the API, but no login data is saved. Without login to authenticate the user, the API denies the mobile app from sending requests to the server.
	Potential fixes for this problem would include either implementing cookie support in the android app or modifying every single endpoint in the API to accept query strings instead of JSON requests. The app would have to continuously send its username to every endpoint. Neither of these solutions were able to be implemented before the due date of the project as the two applications were tested working separately and then failed to work together the night before the due date.

